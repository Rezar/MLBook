{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac8cc9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function:\n",
      "[[65.607346 72.89734  80.997345 89.997345]\n",
      " [59.046345  0.       89.997345 99.997345]\n",
      " [65.607346 72.89734  80.997345 89.997345]\n",
      " [59.046345 65.607346 72.89734  80.997345]]\n",
      "\n",
      "Optimal Policy:\n",
      "[['R' 'R' 'D' 'D']\n",
      " ['U' 'W' 'R' 'R']\n",
      " ['R' 'R' 'U' 'U']\n",
      " ['U' 'U' 'U' 'U']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/_4_3bkv156zfzhgs_qdg_3rw0000gp/T/ipykernel_53368/626287522.py:44: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  optimal_policy = np.zeros_like(grid_world, dtype=np.str)\n"
     ]
    }
   ],
   "source": [
    "# Bellmann equation implementation for Gridworld problem\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the grid-world environment\n",
    "grid_world = np.array([\n",
    "    [0, 0, 0, 0],\n",
    "    [0, -1, 0, 0],\n",
    "    [0, 0, 0, 0],\n",
    "    [0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "# Define the rewards for each cell\n",
    "rewards = np.array([\n",
    "    [0, 0, 0, 0],\n",
    "    [0, 0, 0, 10],\n",
    "    [0, 0, 0, 0],\n",
    "    [0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "# Define the discount factor\n",
    "discount_factor = 0.9\n",
    "\n",
    "# Define the number of iterations for the Bellman equation\n",
    "num_iterations = 100\n",
    "\n",
    "# Initialize the value function array\n",
    "value_function = np.zeros_like(grid_world, dtype=np.float32)\n",
    "\n",
    "# Perform value iteration\n",
    "for _ in range(num_iterations):\n",
    "    updated_value_function = np.copy(value_function)\n",
    "    for i in range(grid_world.shape[0]):\n",
    "        for j in range(grid_world.shape[1]):\n",
    "            if grid_world[i, j] == -1:  # Skip walls or obstacles\n",
    "                continue\n",
    "            up_value = value_function[max(i - 1, 0), j]\n",
    "            down_value = value_function[min(i + 1, grid_world.shape[0] - 1), j]\n",
    "            left_value = value_function[i, max(j - 1, 0)]\n",
    "            right_value = value_function[i, min(j + 1, grid_world.shape[1] - 1)]\n",
    "            max_value = max(up_value, down_value, left_value, right_value)\n",
    "            updated_value_function[i, j] = rewards[i, j] + discount_factor * max_value\n",
    "    value_function = updated_value_function\n",
    "\n",
    "# Find the optimal policy\n",
    "optimal_policy = np.zeros_like(grid_world, dtype=np.str)\n",
    "for i in range(grid_world.shape[0]):\n",
    "    for j in range(grid_world.shape[1]):\n",
    "        if grid_world[i, j] == -1:\n",
    "            optimal_policy[i, j] = \"W\"  # Mark walls or obstacles\n",
    "        else:\n",
    "            up_value = value_function[max(i - 1, 0), j]\n",
    "            down_value = value_function[min(i + 1, grid_world.shape[0] - 1), j]\n",
    "            left_value = value_function[i, max(j - 1, 0)]\n",
    "            right_value = value_function[i, min(j + 1, grid_world.shape[1] - 1)]\n",
    "            max_value = max(up_value, down_value, left_value, right_value)\n",
    "            if max_value == up_value:\n",
    "                optimal_policy[i, j] = \"U\"\n",
    "            elif max_value == down_value:\n",
    "                optimal_policy[i, j] = \"D\"\n",
    "            elif max_value == left_value:\n",
    "                optimal_policy[i, j] = \"L\"\n",
    "            elif max_value == right_value:\n",
    "                optimal_policy[i, j] = \"R\"\n",
    "\n",
    "# Print the results\n",
    "print(\"Optimal Value Function:\")\n",
    "print(value_function)\n",
    "print(\"\\nOptimal Policy:\")\n",
    "print(optimal_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec985225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy: 0, Action: Endurance Training\n",
      "Energy: 1, Action: Endurance Training\n",
      "Energy: 2, Action: Endurance Training\n",
      "Energy: 3, Action: Endurance Training\n",
      "Energy: 4, Action: Endurance Training\n",
      "Energy: 5, Action: Endurance Training\n",
      "Energy: 6, Action: Endurance Training\n",
      "Energy: 7, Action: Endurance Training\n",
      "Energy: 8, Action: Endurance Training\n",
      "Energy: 9, Action: Endurance Training\n",
      "Energy: 10, Action: Endurance Training\n"
     ]
    }
   ],
   "source": [
    "# Bellman equation for fitness coach AI\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "max_energy = 10  # Maximum energy level\n",
    "num_states = max_energy + 1  # Number of states (energy levels)\n",
    "num_actions = 3  # Number of actions (performance training, endurance training, rest)\n",
    "discount_factor = 0.9  # Discount factor\n",
    "num_iterations = 100  # Number of iterations for value iteration\n",
    "\n",
    "# Initialize the value function with zeros\n",
    "V = np.zeros(num_states)\n",
    "\n",
    "# Bellman equation\n",
    "def bellman_equation(state, action):\n",
    "    if action == 0:  # Performance training (jogging or swimming)\n",
    "        next_state = min(state - 2 , max_energy)  # Increase energy by 2\n",
    "        reward = 1  # Positive reward for performance training\n",
    "    elif action == 1:  # Endurance training (weight lifting)\n",
    "        next_state = max(state - 1, 0)  # Decrease energy by 1\n",
    "        reward = 2  # Higher reward for endurance training\n",
    "    else:  # Rest\n",
    "        next_state = state  # Energy level remains the same\n",
    "        reward = 0  # No additional reward for resting\n",
    "\n",
    "    return next_state, reward\n",
    "\n",
    "# Value iteration\n",
    "for _ in range(num_iterations):\n",
    "    new_V = np.zeros(num_states)\n",
    "    for state in range(num_states):\n",
    "        max_value = float('-inf')\n",
    "        for action in range(num_actions):\n",
    "            next_state, reward = bellman_equation(state, action)\n",
    "            value = reward + discount_factor * V[next_state]\n",
    "            max_value = max(max_value, value)\n",
    "        new_V[state] = max_value\n",
    "    V = new_V\n",
    "\n",
    "# Optimal policy extraction\n",
    "policy = np.zeros(num_states, dtype=int)\n",
    "for state in range(num_states):\n",
    "    max_value = float('-inf')\n",
    "    best_action = None\n",
    "    for action in range(num_actions):\n",
    "        next_state, reward = bellman_equation(state, action)\n",
    "        value = reward + discount_factor * V[next_state]\n",
    "        if value > max_value:\n",
    "            max_value = value\n",
    "            best_action = action\n",
    "    policy[state] = best_action\n",
    "\n",
    "# Print the optimal policy\n",
    "actions = ['Performance Training', 'Endurance Training', 'Rest']\n",
    "for state, action in enumerate(policy):\n",
    "    print(f'Energy: {state}, Action: {actions[action]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb75640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
